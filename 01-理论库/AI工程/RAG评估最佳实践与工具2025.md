# RAG评估

## 本质
RAG评估是一套用于衡量和优化检索增强生成模型性能的系统方法，旨在确保AI应用从检索到生成的整个流程都能输出准确、相关且连贯的结果。

## 原理
RAG模型结合了信息检索和文本生成两个阶段，其评估的复杂性源于需要对“双系统”进行协同度量。
*   **推断原理（基于内容）**：评估必须同时覆盖**检索质量**（找到的信息是否相关、全面）和**生成质量**（回答是否准确、流畅、基于检索内容）。这符合**系统论**思想，即整体性能取决于各子系统及其交互，而非单一环节的优劣。
*   **行为经济学原理**：引入**人工评估**作为关键环节，反映了在复杂、模糊（如“有用性”、“相关性”）的质量判断上，人类直觉和领域知识往往优于纯自动化指标，这类似于决策中的“启发式”判断。

## 案例
**参考案例**
*   **背景**：一家金融科技公司部署了一个基于RAG的智能客服助手，用于回答产品条款和投资相关问题。
*   **做法**：团队建立了一个评估框架，结合自动化指标与人工评审。
    1.  **检索评估**：使用NDCG（归一化折损累计增益）分数评估检索文档的排序相关性。
    2.  **生成评估**：使用RAGAS等工具自动评估生成答案的忠实度（是否基于检索内容）和答案相关性。
    3.  **人工评估**：每周由领域专家对100条随机对话进行评分，评估答案的准确性、完整性和实用性。
*   **结果**：经过两轮迭代优化，该助手的用户满意度（CSAT）从68%提升至85%，同时错误答案（幻觉）率降低了40%。

## 行动
1.  **第一步（今天就能做）**：**定义核心评估指标**。为你的RAG应用确定2-3个最关键的成功指标。例如，如果注重事实准确性，可设定“答案忠实度”；如果注重用户体验，可设定“答案相关性”和“流畅度”。
2.  **第二步**：**搭建自动化评估基线**。使用如RAGAS、TruLens等开源评估框架，针对一小部分（如50-100条）带有标准答案的测试问题，运行你的RAG管道，获取初始的检索和生成各项分数，建立性能基线。
3.  **第三步**：**引入人工评估循环**。定期（如每周）抽样一批模型输出，让领域专家或资深用户根据第一步定义的指标进行评分。将人工评估结果与自动化指标对比，校准自动化评估体系，并发现自动化指标无法捕捉的深层次问题。

## 边界
*   ✅ **适用场景**：开发或优化任何基于RAG架构的AI应用（如智能客服、知识库问答、内容摘要）；需要量化模型迭代效果时；在将RAG系统部署到生产环境前进行质量验证。
*   ❌ **不适用场景**：评估不涉及外部知识检索的纯生成式语言模型；对实时性要求极高、完全无法容忍评估延迟的线上推理场景（评估通常离线进行）。
*   ⚠️ **注意事项**：自动化评估指标（如BLEU, ROUGE）可能无法完全反映答案的实际有用性或事实正确性，需与人工评估结合。评估成本（尤其是人工评估）需与项目阶段和重要性平衡。评估数据集的质量和代表性直接决定评估结果的可信度。

---
> 来源：https://orq.ai/blog/rag-evaluation
> 整理：Mr.Chen
> 日期：2024-05-23