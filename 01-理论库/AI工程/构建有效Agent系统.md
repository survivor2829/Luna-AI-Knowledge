---
整理：Mr.Chen
原文日期：2024-12-19
更新日期：2025-12-30
原文链接：https://www.anthropic.com/research/building-effective-agents
---

# 构建有效的Agent系统

> 类型：AI工程
> 难度：进阶

## 核心问题

**如何在"能力"与"可控性"之间找到最优平衡点？**

Agent系统的本质困境：LLM越自主，能力越强，但可控性和可预测性越低。这篇文章要解决的根本问题是——如何用最小的复杂度获得足够的能力。

## 设计哲学

### 简单性优先原则

> "最成功的实现使用简单、可组合的模式，而非复杂的框架。"

这不是保守，而是工程智慧：

1. **复杂度是负债，不是资产** —— 每增加一层抽象，就多一个故障点、多一个调试盲区
2. **框架的陷阱** —— 框架"往往创建额外的抽象层，会遮蔽底层的提示词"，你失去了对系统最关键部分的直接控制
3. **逐步升级** —— 先优化单次LLM调用（检索+上下文示例），只有当简单方案明显不足时才增加复杂度

## 底层原理

### 原理1：确定性与非确定性的边界设计

Agent系统的核心张力在于：**确定性代码**与**非确定性LLM**的协作。

- **Workflow（工作流）**：LLM被编排在预定义的代码路径中 —— 人类掌控流程，LLM负责理解和生成
- **Agent（智能体）**：LLM动态决定流程和工具使用 —— LLM掌控流程，代码提供能力边界

选择哪种模式，取决于**任务的开放程度**和**错误的容忍度**。

### 原理2：工具定义是契约设计

工具不只是"给LLM用的函数"，本质上是**在非确定性系统和确定性系统之间建立可靠的通信协议**。

- 工具描述的质量直接决定LLM能否正确使用
- 格式选择影响LLM的理解（JSON需要转义，Markdown更易读）
- 这就是为什么Anthropic强调"精心设计Agent-Computer Interface (ACI)"

### 原理3：反馈循环决定自主上限

Agent能处理开放性问题的关键在于**环境反馈**：

- 编码Agent通过自动化测试获得反馈 —— 测试结果是明确的对错信号
- 客服Agent通过用户响应获得反馈 —— 但信号更模糊，需要更多约束

**没有可靠反馈的领域，不适合高度自主的Agent。**

## 架构思维

### 构建模块的组合哲学

Anthropic提出的不是"最佳架构"，而是**可组合的构建模块**：

```
┌─────────────────────────────────────────────────────────┐
│                    复杂度递增方向                         │
├──────────┬──────────┬──────────┬──────────┬────────────┤
│ 增强型LLM │ 提示链   │  路由    │ 并行化   │ 编排-工作者 │
│          │          │          │          │            │
│ 基础能力  │ 分解任务 │ 分类分流 │ 同时处理 │ 动态委托   │
│ +工具    │ +门控检查│ +专业化  │ +投票    │ +综合      │
└──────────┴──────────┴──────────┴──────────┴────────────┘
```

**每个模块解决特定问题：**

| 模式 | 解决的问题 | 代价 |
|------|-----------|------|
| 提示链(Prompt Chaining) | 复杂任务分解 | 延迟增加 |
| 路由(Routing) | 输入多样性 | 分类可能出错 |
| 并行化(Parallelization) | 效率/置信度 | 成本倍增 |
| 编排-工作者(Orchestrator-Workers) | 动态子任务 | 控制流复杂 |
| 评估-优化器(Evaluator-Optimizer) | 迭代改进 | 循环可能不收敛 |

## 关键实现

### 门控检查：Workflow的可靠性保障

```python
# 提示链中的"门控"设计
# 本质：在LLM步骤之间插入确定性检查点

def process_with_gates(input):
    # Step 1: LLM处理
    result_1 = llm_step_1(input)

    # Gate 1: 程序化验证（不是LLM判断）
    if not validate_format(result_1):
        return handle_error("格式验证失败")

    # Step 2: 只有通过门控才继续
    result_2 = llm_step_2(result_1)

    # Gate 2: 业务规则检查
    if not business_rule_check(result_2):
        return handle_error("业务规则不满足")

    return result_2

# 设计哲学：用确定性代码约束非确定性LLM
# 每个Gate是一个"检查点"，防止错误传播
```

### 工具格式的可读性优化

```python
# 错误做法：JSON中的代码需要大量转义
{
    "code": "def hello():\n    print(\"Hello\")\n    return True"
}

# 正确做法：使用Markdown，保持代码原貌
"""
```python
def hello():
    print("Hello")
    return True
```
"""

# 原理：减少格式噪音，让模型专注于内容理解
```

## 实践要点

1. **从单次调用开始** —— 90%的任务不需要Agent，优化好的单次调用+检索就够了。这不是偷懒，是尊重简单性原则。

2. **透明性设计** —— 让Agent显式输出其思考过程（规划步骤），不是为了好看，而是为了：可调试、可干预、可信任。

3. **工具文档是第一等公民** —— 你在工具描述上花的时间，会在LLM正确调用率上得到回报。这是投资回报率最高的优化点。

4. **选择有明确反馈的领域** —— 编码任务成功率高，因为测试提供了无歧义的反馈。如果你的领域没有这种反馈，要么创造它，要么降低自主度。

## 设计权衡

| 选择 | 获得 | 牺牲 |
|------|------|------|
| 用Workflow而非Agent | 可控性、可预测性、可调试性 | 灵活性、处理意外的能力 |
| 用Agent而非Workflow | 开放问题处理能力、自主性 | 可预测性、调试难度增加 |
| 简单架构 | 可维护性、故障排查容易 | 可能无法处理复杂场景 |
| 复杂架构 | 理论上更强大 | 更多故障点、更难优化 |

**Anthropic的建议本质：承担复杂度的举证责任。**

不是"能用复杂架构就用"，而是"除非能证明简单方案不行，否则用简单方案"。

## 完整内容

### 什么是Agent？

Anthropic区分两类系统：
- **Workflow**：LLM和工具通过预定义代码路径编排
- **Agent**：LLM动态指导流程和工具使用

### 何时使用Agent？

核心建议：从简单开始。"优化单次LLM调用配合检索和上下文示例通常就足够了。"只有当复杂度能带来明确改进时才增加。

### 关于框架的警告

列举的工具包括Claude Agent SDK、Strands Agents SDK、Rivet和Vellum。但关键警告是：框架"往往创建额外的抽象层，可能遮蔽底层提示词"。

### 构建模块模式

1. **增强型LLM** —— 基础：结合检索、工具和记忆，通过Model Context Protocol集成
2. **提示链** —— 将任务分解为顺序步骤，步骤间有程序化检查（门控）
3. **路由** —— 将输入分类到专门的下游处理流程
4. **并行化** —— 两种变体：分段（独立并行子任务）和投票（多次尝试提高置信度）
5. **编排者-工作者** —— 中央LLM动态委托任务给工作者LLM，综合结果
6. **评估者-优化器** —— 迭代循环：一个LLM生成，另一个评估并提供反馈

### 三个核心实现原则

1. 保持设计简单
2. 通过显式规划步骤优先考虑透明性
3. 通过详尽的工具文档精心设计Agent-Computer Interface (ACI)

### 实际应用案例

**客户支持**：自然适合结合聊天机器人和数据检索、程序化操作（退款、工单更新）的工具。一些公司使用基于使用量的定价来证明效果。

**编码Agent**：通过自动化测试验证解决方案；Agent使用测试反馈迭代。Anthropic的实现通过SWE-bench Verified基准测试解决真实GitHub问题。

### 工具工程

格式选择很重要："在JSON中编写代码需要额外转义换行符和引号"。建议强调模型的可读性，避免不必要的格式开销。

### 核心结论

成功需要选择正确的系统复杂度级别。先构建基础提示词，充分评估，只有当简单方案明显不足时才添加多步Agent系统。
